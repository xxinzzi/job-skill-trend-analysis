import sys
import os
import time
import uuid
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# utilsÏóêÏÑú MongoDB Ïú†Ìã∏ Î∂àÎü¨Ïò§Í∏∞
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from utils.mongo_utils import init_mongo, get_collection

# MongoDB Ï¥àÍ∏∞Ìôî Î∞è Ïª¨Î†âÏÖò Í∞ùÏ≤¥
init_mongo()
raw_col = get_collection("raw_postings")

# Selenium ÌÅ¨Î°¨ ÎìúÎùºÏù¥Î≤Ñ ÏòµÏÖò
options = Options()
# options.add_argument("--headless")  # ÌïÑÏöî Ïãú ÌôúÏÑ±Ìôî
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
driver = webdriver.Chrome(options=options)
wait = WebDriverWait(driver, 10)

# Ïû°ÏΩîÎ¶¨ÏïÑ Ï±ÑÏö© ÌéòÏù¥ÏßÄ ÏßÑÏûÖ
driver.get("https://www.jobkorea.co.kr/recruit/joblist?menucode=duty")

# ÎåÄÎ∂ÑÎ•ò ÏÑ†ÌÉù (AI¬∑Í∞úÎ∞ú¬∑Îç∞Ïù¥ÌÑ∞)
label = driver.find_element(By.CSS_SELECTOR, "label[for='duty_step1_10031']")
driver.execute_script("arguments[0].click();", label)

# Ï§ëÎ∂ÑÎ•ò Î¶¨Ïä§Ìä∏
wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul#duty_step2_10031_ly li input[type='checkbox']")))
mid_categories = driver.find_elements(By.CSS_SELECTOR, "ul#duty_step2_10031_ly li input[type='checkbox']")

# ÏÇ¨Ïö©ÏûêÎ°úÎ∂ÄÌÑ∞ Ï§ëÎ∂ÑÎ•ò ÏÑ†ÌÉù
print(f"\nüîé Ï¥ù {len(mid_categories)}Í∞úÏùò Ï§ëÎ∂ÑÎ•òÍ∞Ä ÏûàÏäµÎãàÎã§.")
for i, cat in enumerate(mid_categories):
    label = driver.find_element(By.CSS_SELECTOR, f"label[for='{cat.get_attribute('id')}']").text.strip()
    print(f"[{i}] {label}")
target_idx = int(input("\nüü° ÏàòÏßëÌï† Ï§ëÎ∂ÑÎ•ò Ïù∏Îç±Ïä§ Î≤àÌò∏Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî: "))

# ÏÑ†ÌÉùÎêú Ï§ëÎ∂ÑÎ•òÎßå ÏàòÏßë
try:
    mid_cat = mid_categories[target_idx]
    mid_cat_id = mid_cat.get_attribute("id")
    mid_cat_value = mid_cat.get_attribute("value")
    mid_cat_label = driver.find_element(By.CSS_SELECTOR, f"label[for='{mid_cat_id}']").text.strip()

    driver.execute_script("arguments[0].scrollIntoView(true);", mid_cat)
    time.sleep(0.5)
    driver.execute_script("arguments[0].click();", mid_cat)
    time.sleep(2)

    # Í≤ÄÏÉâ Î≤ÑÌäº ÌÅ¥Î¶≠ ÌõÑ Î°úÎî©
    search_btn = wait.until(EC.element_to_be_clickable((By.ID, "dev-btn-search")))
    driver.execute_script("arguments[0].click();", search_btn)
    time.sleep(3)

    # Í≥µÍ≥† Ïàò Ï∂îÏ∂ú: 'AI/ML ÏóîÏßÄÎãàÏñ¥ (418)' ‚Üí 418
    match = re.search(r"\((\d+)\)", mid_cat_label)
    if match:
        total_count = int(match.group(1))
    else:
        total_count = 0

    # ÌéòÏù¥ÏßÄ Ïàò Í≥ÑÏÇ∞
    total_pages = (total_count + 39) // 40
    mid_cat_clean_label = re.sub(r"\s*\(.*?\)", "", mid_cat_label).strip()

    print(f"\nÏ¥ù {total_count}Í±¥ ‚Üí {total_pages} ÌéòÏù¥ÏßÄÏóêÏÑú ÏàòÏßëÌï©ÎãàÎã§.")

    for page in range(1, total_pages + 1):
        print(f"\n‚è≥ {page} ÌéòÏù¥ÏßÄ ÏàòÏßë Ï§ë...")

        page_url = f"https://www.jobkorea.co.kr/recruit/joblist?menucode=duty&duty1=10031&duty2={mid_cat_value}&page={page}"
        driver.get(page_url)
        time.sleep(3)

        dev_gi_list = wait.until(EC.presence_of_element_located((By.ID, "dev-gi-list")))
        postings = dev_gi_list.find_elements(By.CSS_SELECTOR, "tr.devloopArea")

        for post in postings:
            try:
                title_elem = post.find_element(By.CSS_SELECTOR, "td.tplTit strong a")
                link = title_elem.get_attribute("href")

                # ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄ Ïù¥Îèô
                driver.execute_script("window.open(arguments[0]);", link)
                driver.switch_to.window(driver.window_handles[-1])
                time.sleep(2)

                soup = BeautifulSoup(driver.page_source, "html.parser")

                # Íµ¨Ï°∞1 ÌÉêÏßÄ
                structure1 = soup.select_one("div.tbCol.tbCoInfo") or soup.select_one("div.tbCol.tbCoResume")
                # Íµ¨Ï°∞2 ÌÉêÏßÄ
                structure2 = soup.select_one("div.dev-wrap-detailContents") or soup.select_one("div.recruit-data")

                raw_blocks = {}
                raw_text_blocks = {}
                title = company = ""

                # Íµ¨Ï°∞1Ïùº Í≤ΩÏö∞
                if structure1:
                    header_div = soup.select_one("h3.hd_3 > div.header")
                    company = header_div.select_one("span.coName").text.strip() if header_div and header_div.select_one("span.coName") else ""

                    title_tag = soup.select_one("h3.hd_3")
                    if title_tag:
                        for tag in title_tag.find_all(['div', 'span']):
                            tag.decompose()
                        title = title_tag.get_text(strip=True)

                    raw_blocks["corp_info_block"] = str(soup.select_one("div.tbCol.tbCoInfo"))
                    raw_blocks["requirements_block"] = str(soup.select_one("div.tbCol.tbCoResume"))

                    job_desc = soup.select_one("div.artRead_detail")
                    raw_blocks["job_description_block"] = {
                        "html": str(job_desc),
                        "image_urls": [img["src"] for img in job_desc.find_all("img") if img.get("src")]
                    } if job_desc else {}

                    raw_text_blocks["corp_info_text"] = soup.select_one("div.tbCol.tbCoInfo").get_text(strip=True) if soup.select_one("div.tbCol.tbCoInfo") else ""
                    raw_text_blocks["requirements_text"] = soup.select_one("div.tbCol.tbCoResume").get_text(strip=True) if soup.select_one("div.tbCol.tbCoResume") else ""
                    raw_text_blocks["job_description_text"] = job_desc.get_text(strip=True) if job_desc else ""

                # Íµ¨Ï°∞2Ïùº Í≤ΩÏö∞
                elif structure2:
                    title = soup.select_one("h2.title-recruit").text.strip() if soup.select_one("h2.title-recruit") else ""
                    company = soup.select_one("a.devTitleCoReadUrl").text.strip() if soup.select_one("a.devTitleCoReadUrl") else ""

                    raw_blocks["corp_info_block"] = str(soup.select_one("article.devCompanyInfo"))
                    raw_blocks["requirements_block"] = str(soup.select_one("div.recruit-data"))

                    job_desc = soup.select_one("div.dev-wrap-detailContents")
                    raw_blocks["job_description_block"] = {
                        "html": str(job_desc),
                        "image_urls": [img["src"] for img in job_desc.find_all("img") if img.get("src")]
                    } if job_desc else {}

                    raw_text_blocks["corp_info_text"] = soup.select_one("article.devCompanyInfo").get_text(strip=True) if soup.select_one("article.devCompanyInfo") else ""
                    raw_text_blocks["requirements_text"] = soup.select_one("div.recruit-data").get_text(strip=True) if soup.select_one("div.recruit-data") else ""
                    raw_text_blocks["job_description_text"] = job_desc.get_text(strip=True) if job_desc else ""

                document = {
                    "url": link,
                    "source": "jobkorea",
                    "title": title,
                    "company": company,
                    "job_category": mid_cat_clean_label,
                    "raw_html_blocks": raw_blocks,
                    "raw_text_blocks": raw_text_blocks,
                    "crawl_id": str(uuid.uuid4()),
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }

                raw_col.insert_one(document)
                print("‚úÖ Ï†ÄÏû• ÏôÑÎ£å:", title)

                driver.close()
                driver.switch_to.window(driver.window_handles[0])

            except Exception as post_err:
                print("‚ùå Í≥µÍ≥† ÌååÏã± Ïã§Ìå®:", post_err)
                driver.close()
                driver.switch_to.window(driver.window_handles[0])
                continue

except Exception as e:
    print("‚ùå Ï§ëÎ∂ÑÎ•ò Ï≤òÎ¶¨ Ïã§Ìå®:", e)

driver.quit()