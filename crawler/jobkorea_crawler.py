import sys
import os
import time
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import random

# utilsì—ì„œ MongoDB ë° S3 ì—…ë¡œë“œ ìœ í‹¸ ë¶ˆëŸ¬ì˜¤ê¸°
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from utils.mongo_utils import init_mongo, get_collection
from utils.s3_utils import upload_image_to_s3
from utils.captcha_utils import check_and_wait_for_captcha

# MongoDB ì´ˆê¸°í™” ë° ì»¬ë ‰ì…˜ ê°ì²´
init_mongo()
raw_col = get_collection("raw_postings_jobkorea_test")

# Selenium í¬ë¡¬ ë“œë¼ì´ë²„ ì˜µì…˜
options = Options()
# options.add_argument("--headless")  # í•„ìš” ì‹œ í™œì„±í™”
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
driver = webdriver.Chrome(options=options)
wait = WebDriverWait(driver, 10)

# ì¡ì½”ë¦¬ì•„ ì±„ìš© í˜ì´ì§€ ì§„ì…
driver.get("https://www.jobkorea.co.kr/recruit/joblist?menucode=duty")

# ëŒ€ë¶„ë¥˜ ì„ íƒ (AIÂ·ê°œë°œÂ·ë°ì´í„°)
label = driver.find_element(By.CSS_SELECTOR, "label[for='duty_step1_10031']")
driver.execute_script("arguments[0].click();", label)

# ì¤‘ë¶„ë¥˜ ë¦¬ìŠ¤íŠ¸
wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul#duty_step2_10031_ly li input[type='checkbox']")))
mid_categories = driver.find_elements(By.CSS_SELECTOR, "ul#duty_step2_10031_ly li input[type='checkbox']")

# ì‚¬ìš©ìë¡œë¶€í„° ì¤‘ë¶„ë¥˜ ì„ íƒ
for i, cat in enumerate(mid_categories):
    label = driver.find_element(By.CSS_SELECTOR, f"label[for='{cat.get_attribute('id')}']").text.strip()
    print(f"[{i}] {label}")
target_idx = int(input("\nğŸŸ¡ ìˆ˜ì§‘í•  ì¤‘ë¶„ë¥˜ ì¸ë±ìŠ¤ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: "))

# ì„ íƒëœ ì¤‘ë¶„ë¥˜ë§Œ ìˆ˜ì§‘
try:
    mid_cat = mid_categories[target_idx]
    mid_cat_id = mid_cat.get_attribute("id")
    mid_cat_value = mid_cat.get_attribute("value")
    mid_cat_label = driver.find_element(By.CSS_SELECTOR, f"label[for='{mid_cat_id}']").text.strip()

    driver.execute_script("arguments[0].scrollIntoView(true);", mid_cat)
    time.sleep(0.5)
    driver.execute_script("arguments[0].click();", mid_cat)
    time.sleep(2)

    # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ í›„ ë¡œë”©
    search_btn = wait.until(EC.element_to_be_clickable((By.ID, "dev-btn-search")))
    driver.execute_script("arguments[0].click();", search_btn)
    time.sleep(3)

    # ê³µê³  ìˆ˜ ì¶”ì¶œ
    match = re.search(r"\((\d+)\)", mid_cat_label)
    if match:
        total_count = int(match.group(1))
    else:
        total_count = 0

    # í˜ì´ì§€ ìˆ˜ ê³„ì‚°
    total_pages = (total_count + 39) // 40
    mid_cat_clean_label = re.sub(r"\s*\(.*?\)", "", mid_cat_label).strip()

    start_page = int(input("ì´ í˜ì´ì§€: {total_pages}\nì‹œì‘í•  í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: "))

    # start_pageë¡œ ì´ë™
    if start_page > 1:
        for i in range(1, start_page):
            if i % 10 == 0:
                next_group_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "a.btnPgnNext")))
                driver.execute_script("arguments[0].click();", next_group_btn)
            else:
                page_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, f"a[data-page='{i+1}']")))
                driver.execute_script("arguments[0].click();", page_btn)
        time.sleep(1)

    current_page = start_page

    while current_page <= total_pages:
        print(f"\nâ³ {current_page}/{total_pages} í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")

        # ê³µê³  ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
        dev_gi_list = wait.until(EC.presence_of_element_located((By.ID, "dev-gi-list")))
        postings = dev_gi_list.find_elements(By.CSS_SELECTOR, "tr.devloopArea")

        for post in postings:
            # CAPTCHA ê°ì§€ ë° ì˜ˆì™¸ ì²˜ë¦¬
            check_and_wait_for_captcha(driver, current_page)

            title_elem = post.find_element(By.CSS_SELECTOR, "td.tplTit strong a")
            link = title_elem.get_attribute("href")

            try:
                title_elem = post.find_element(By.CSS_SELECTOR, "td.tplTit strong a")
                link = title_elem.get_attribute("href")

                # ìƒì„¸ í˜ì´ì§€ ì´ë™
                driver.execute_script("window.open(arguments[0]);", link)
                driver.switch_to.window(driver.window_handles[-1])
                time.sleep(2)

                soup = BeautifulSoup(driver.page_source, "html.parser")

                # êµ¬ì¡°1 íƒì§€
                structure1 = soup.select_one("div.tbCol.tbCoInfo") or soup.select_one("div.tbCol.tbCoResume")
                # êµ¬ì¡°2 íƒì§€
                structure2 = soup.select_one("div.dev-wrap-detailContents") or soup.select_one("div.recruit-data")

                title = company = ""

                # ê³µí†µ í•„ë“œ ë¨¼ì € êµ¬ì„±
                document = {
                    "url": link,
                    "source": "jobkorea",
                    "job_category": "AIÂ·ê°œë°œÂ·ë°ì´í„°",
                    "job_mid_category": mid_cat_clean_label,
                }

                # êµ¬ì¡°1ì¼ ê²½ìš°
                if structure1:
                    header_div = soup.select_one("h3.hd_3 > div.header")
                    company = header_div.select_one("span.coName").text.strip() if header_div and header_div.select_one("span.coName") else ""

                    title_tag = soup.select_one("h3.hd_3")
                    if title_tag:
                        for tag in title_tag.find_all(['div', 'span']):
                            tag.decompose()
                        title = title_tag.get_text(strip=True)

                    job_summary = soup.select_one("article.artReadJobSum div.tbRow.clear")
                    detail_section = soup.select_one("section#tab01.secReadDetail")
                    
                    document.update({
                        "title": title,
                        "company": company,
                        "jobsum_text": job_summary.get_text(separator="\n", strip=True) if job_summary else "",
                        "detail_text": detail_section.get_text(separator="\n", strip=True) if detail_section else ""
                    })

                    iframe_text = ""
                    image_urls = []

                    try:
                        iframe_elem = driver.find_element(By.CSS_SELECTOR, "iframe#gib_frame")
                        driver.switch_to.frame(iframe_elem)

                        # iframe ì „ì²´ HTML íŒŒì‹±
                        iframe_soup = BeautifulSoup(driver.page_source, "html.parser")

                        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        content_div = iframe_soup.select_one("div.secDetailWrap")
                        if content_div:
                            iframe_text = content_div.get_text(separator="\n", strip=True)

                        # ì´ë¯¸ì§€ ì¶”ì¶œ
                        iframe_images = iframe_soup.find_all("img")
                        image_urls = [img.get("src") for img in iframe_images if img.get("src")]

                        driver.switch_to.default_content()

                    except Exception as iframe_err:
                        print("âŒ iframe ì²˜ë¦¬ ì‹¤íŒ¨:", iframe_err)

                    if iframe_text:
                        document["iframe_text"] = iframe_text

                    if image_urls:
                        s3_urls = []
                        for url in image_urls:
                            s3_url = upload_image_to_s3(url, "jobkorea")
                            if s3_url:
                                s3_urls.append(s3_url)

                        if s3_urls:
                            document["image_urls"] = s3_urls
                        else:
                            # ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì›ë³¸ URL fallback
                            s3_urls.append(url)

                # êµ¬ì¡°2ì¼ ê²½ìš°
                elif structure2:
                    title = soup.select_one("h2.title-recruit").text.strip() if soup.select_one("h2.title-recruit") else ""
                    company = soup.select_one("a.devTitleCoReadUrl").text.strip() if soup.select_one("a.devTitleCoReadUrl") else ""

                    section = soup.select_one("section.section-content")
                    aside = soup.select_one("section.aside")

                    document.update({
                        "title": title,
                        "company": company,
                        "section_text": section.get_text(separator="\n", strip=True) if section else "",
                        "aside_text": aside.get_text(separator="\n", strip=True) if aside else ""
                    })

                raw_col.insert_one(document)
                print("âœ… ì €ì¥ ì™„ë£Œ:", title)

                driver.close()
                driver.switch_to.window(driver.window_handles[0])

            except Exception as post_err:
                print("âŒ ê³µê³  íŒŒì‹± ì‹¤íŒ¨:", post_err)
                driver.close()
                driver.switch_to.window(driver.window_handles[0])
                continue
        
        # IP ì°¨ë‹¨ ë°©ì§€ ì†ë„ ì¡°ì ˆ
        time.sleep(random.uniform(2, 5))

        # ë‹¤ìŒ í˜ì´ì§€ í´ë¦­
        current_page += 1
        if current_page > total_pages:
            break
        if current_page % 10 == 1:  # 11, 21, 31... â†’ "ë‹¤ìŒ" ë²„íŠ¼ ëˆŒëŸ¬ì•¼ í•¨
            try:
                next_group_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "a.btnPgnNext")))
                driver.execute_script("arguments[0].click();", next_group_btn)
                time.sleep(2)
            except Exception as e:
                print("âŒ ë‹¤ìŒ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨:", e)
                break
        else:
            try:
                next_page_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, f"a[data-page='{current_page}']")))
                driver.execute_script("arguments[0].click();", next_page_btn)
                time.sleep(2)
            except Exception as e:
                print(f"âŒ {current_page} í˜ì´ì§€ë¡œ ì´ë™ ì‹¤íŒ¨:", e)
                break

except Exception as e:
    print(f"\nâŒ ì¤‘ë‹¨ë¨: {e}")
    print(f"ì¤‘ë‹¨ëœ í˜ì´ì§€ ë²ˆí˜¸: {current_page}")
    driver.quit()
    sys.exit(1)  # ê°•ì œ ì¢…ë£Œ

driver.quit()